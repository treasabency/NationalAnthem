install.packages("dplyr")
install.packages("tidytext")
install.packages("tm")
install.packages("stringr")
install.packages("proxy")
install.packages("ggplot2")
install.packages("cluster")
install.packages("xfun")
install.packages("factoextra")
install.packages("tidyverse")
install.packages("textclean")
install.packages("qdapRegex")
library(qdapRegex)
library(textclean)
library(tidyverse)
library(factoextra)
library(xfun)
library(cluster)
library(ggplot2)
library(proxy)
library(stringr)
library(tm)
library(dplyr)
library(tidytext)

#copy of data source
data <- anthems 

# extract anthems for cleaning
my_func <- function(D){
  x <- list()
  for (k in 1:190){
    x[[k]] <- D[k,5]
  }
  x
}

# data processing
anthem <- my_func(anthems)
corpus <- VectorSource(anthem)
corpus <- Corpus(corpus)
anthem <- tm::tm_map(corpus, tolower)
anthem <- tm::tm_map(anthem, removeWords, stopwords("english"))
stopwords <- head(sort(unlist(stopwords), decreasing=TRUE), 960)
stopwords <- removePunctuation(stopwords)
anthem <- tm::tm_map(anthem, removeWords, stopwords)
anthem <- tm::tm_map(anthem, removePunctuation)
anthem <- tm::tm_map(anthem, stemDocument)
anthem <- tm::tm_map(anthem, removeWords, "http[A-Za-z]+")
anthem <- tm::tm_map(anthem, removeWords, "www.[A-Za-z]+")
anthem <- tm::tm_map(anthem, removeWords, c("afa201rpafad", "aa200o", "a200oeend","listanthem"))
anthem <- tm::tm_map(anthem, rm_non_ascii)
anthem <- tm::tm_map(anthem, rm_nchar_words, 1)
anthem <- tm::tm_map(anthem, rm_nchar_words, 2)

# Td-Idf and K-means clustering
dtm <- DocumentTermMatrix(anthem)
dtm <- removeSparseTerms(dtm, sparse = 0.98)
dtm.tfidf <- weightTfIdf(dtm)
tfidf.matrix <- t(as.matrix(dtm.tfidf)) 
fviz_nbclust(tfidf.matrix, kmeans, method = "wss")  # elbow method to determine best k value
clustering.kmeans <- kmeans(tfidf.matrix, centers = 7)
